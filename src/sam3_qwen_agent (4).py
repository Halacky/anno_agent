"""
SAM3 + Qwen VLM Agent
–ê–≥–µ–Ω—Ç, –≥–¥–µ Qwen VLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è SAM3
–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ sam3_agent.ipynb –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è facebookresearch/sam3
"""

import requests
import base64
import json
import re
from typing import List, Dict, Optional, Union, Tuple
from dataclasses import dataclass
from io import BytesIO
from PIL import Image, ImageDraw, ImageFont
import numpy as np


@dataclass
class BoundingBox:
    """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π bounding box [cx, cy, w, h]"""
    cx: float
    cy: float
    w: float
    h: float
    
    def to_list(self) -> List[float]:
        return [self.cx, self.cy, self.w, self.h]
    
    def to_xyxy(self, width: int, height: int) -> Tuple[int, int, int, int]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã [x1, y1, x2, y2]"""
        x1 = int((self.cx - self.w / 2) * width)
        y1 = int((self.cy - self.h / 2) * height)
        x2 = int((self.cx + self.w / 2) * width)
        y2 = int((self.cy + self.h / 2) * height)
        return (x1, y1, x2, y2)
    
    @classmethod
    def from_xyxy_normalized(cls, x1: float, y1: float, x2: float, y2: float):
        """–°–æ–∑–¥–∞—Ç—å –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç [x1, y1, x2, y2]"""
        cx = (x1 + x2) / 2
        cy = (y1 + y2) / 2
        w = x2 - x1
        h = y2 - y1
        return cls(cx, cy, w, h)
    
    @classmethod
    def from_xyxy_absolute(cls, x1: int, y1: int, x2: int, y2: int, img_width: int, img_height: int):
        """–°–æ–∑–¥–∞—Ç—å –∏–∑ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç [x1, y1, x2, y2] –≤ –ø–∏–∫—Å–µ–ª—è—Ö"""
        x1_norm = x1 / img_width
        y1_norm = y1 / img_height
        x2_norm = x2 / img_width
        y2_norm = y2 / img_height
        return cls.from_xyxy_normalized(x1_norm, y1_norm, x2_norm, y2_norm)


@dataclass
class SAM3Prompt:
    """–ü—Ä–æ–º–ø—Ç –¥–ª—è SAM3, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π MLLM"""
    type: str  # "text", "box", or "point"
    text: Optional[str] = None
    box: Optional[List[float]] = None
    points: Optional[List[List[float]]] = None
    point_labels: Optional[List[int]] = None
    reasoning: Optional[str] = None  # –ü–æ—á–µ–º—É MLLM –≤—ã–±—Ä–∞–ª —ç—Ç–æ—Ç –ø—Ä–æ–º–ø—Ç


@dataclass
class SegmentationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –æ—Ç MLLM"""
    mask: str  # RLE-encoded mask
    bbox: BoundingBox
    score: float
    query: str  # –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    llm_reasoning: Optional[str] = None  # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç MLLM
    prompt_used: Optional[SAM3Prompt] = None


class SAM3QwenAgent:
    """
    SAM3 Agent - MLLM (Qwen) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç SAM3 –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    
    Workflow:
    1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: "segment the leftmost child wearing blue vest"
    2. Qwen –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–∫ –ª—É—á—à–µ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å
    3. Qwen –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç(—ã) –¥–ª—è SAM3 (text –∏–ª–∏ bbox)
    4. SAM3 –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –æ—Ç Qwen
    """
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è MLLM, —á—Ç–æ–±—ã –æ–Ω —Ä–∞–±–æ—Ç–∞–ª –∫–∞–∫ –∞–≥–µ–Ω—Ç SAM3
    AGENT_SYSTEM_PROMPT = """You are a vision AI assistant that uses SAM3 (Segment Anything Model 3) as a tool.

Your task is to analyze images and generate appropriate prompts for SAM3 to segment objects based on user queries.

SAM3 supports three types of prompts:
1. TEXT prompts: Simple text descriptions (e.g., "red car", "person wearing hat")
2. BOX prompts: Bounding boxes [cx, cy, w, h] in normalized coordinates (0-1)
3. POINT prompts: Click points [[x, y], ...] with labels [1 for positive, 0 for negative]

For each user query:
1. Analyze the image carefully
2. Determine the best SAM3 prompt strategy
3. Generate the appropriate prompt(s)
4. Explain your reasoning

Respond in JSON format:
{
  "reasoning": "explanation of your analysis",
  "prompts": [
    {
      "type": "text" | "box" | "point",
      "text": "description" (for text prompts),
      "box": [cx, cy, w, h] (for box prompts),
      "points": [[x, y], ...] (for point prompts),
      "point_labels": [1, 0, ...] (for point prompts)
    }
  ]
}

Remember:
- For complex spatial queries ("leftmost", "behind", "next to"), analyze positions carefully
- For attribute-based queries ("wearing blue", "with stripes"), focus on visual features
- Prefer TEXT prompts for simple objects, BOX prompts when you can localize precisely
- You can generate multiple prompts for multiple objects
"""
    
    def __init__(
        self,
        sam3_url: str = "http://localhost:8000",
        qwen_url: str = "http://localhost:8001",
        sam3_api_version: str = "v1",
        qwen_api_version: str = "v1",
        debug: bool = False
    ):
        self.sam3_url = sam3_url.rstrip('/')
        self.qwen_url = qwen_url.rstrip('/')
        self.sam3_api_version = sam3_api_version
        self.qwen_api_version = qwen_api_version
        self.debug = debug
        
    def _image_to_base64(self, image: Union[str, Image.Image, np.ndarray]) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64"""
        if isinstance(image, str):
            with open(image, 'rb') as f:
                return base64.b64encode(f.read()).decode('utf-8')
        elif isinstance(image, Image.Image):
            buffer = BytesIO()
            image.save(buffer, format='PNG')
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        elif isinstance(image, np.ndarray):
            img = Image.fromarray(image)
            buffer = BytesIO()
            img.save(buffer, format='PNG')
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        else:
            raise ValueError("Unsupported image type")
    
    def _get_image_size(self, image: Union[str, Image.Image, np.ndarray]) -> Tuple[int, int]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (width, height)"""
        if isinstance(image, str):
            with Image.open(image) as img:
                return img.size
        elif isinstance(image, Image.Image):
            return image.size
        elif isinstance(image, np.ndarray):
            return (image.shape[1], image.shape[0])
        else:
            raise ValueError("Unsupported image type")
    
    def _extract_json_from_text(self, text: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ MLLM"""
        # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –º–µ–∂–¥—É ```json –∏ ```
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø—Ä–æ—Å—Ç–æ JSON –æ–±—ä–µ–∫—Ç
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass
        
        return None
    
    def _normalize_bbox_from_llm(self, bbox: List[float], img_width: int, img_height: int) -> List[float]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç bbox –æ—Ç LLM –≤ —Ñ–æ—Ä–º–∞—Ç [cx, cy, w, h] (0-1)
        
        LLM –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å:
        - –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã [x1, y1, x2, y2]
        - –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã [x1, y1, x2, y2]
        - –£–∂–µ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ [cx, cy, w, h]
        """
        if len(bbox) != 4:
            raise ValueError(f"Bbox –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 4 –∑–Ω–∞—á–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–æ {len(bbox)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É–∂–µ –ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è (–≤—Å–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1)
        if all(0 <= v <= 1 for v in bbox):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç
            if bbox[2] < bbox[0] or bbox[3] < bbox[1]:
                # –≠—Ç–æ [cx, cy, w, h]
                return bbox
            else:
                # –≠—Ç–æ [x1, y1, x2, y2], –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ [cx, cy, w, h]
                x1, y1, x2, y2 = bbox
                cx = (x1 + x2) / 2
                cy = (y1 + y2) / 2
                w = x2 - x1
                h = y2 - y1
                return [cx, cy, w, h]
        else:
            # –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç [x1, y1, x2, y2]
            x1, y1, x2, y2 = bbox
            x1_norm = x1 / img_width
            y1_norm = y1 / img_height
            x2_norm = x2 / img_width
            y2_norm = y2 / img_height
            
            cx = (x1_norm + x2_norm) / 2
            cy = (y1_norm + y2_norm) / 2
            w = x2_norm - x1_norm
            h = y2_norm - y1_norm
            
            return [cx, cy, w, h]
    
    def generate_sam3_prompts(
        self,
        image: Union[str, Image.Image, np.ndarray],
        query: str
    ) -> Tuple[List[SAM3Prompt], str]:
        """
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è SAM3
        
        Args:
            image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "the leftmost child wearing blue vest")
            
        Returns:
            Tuple[List[SAM3Prompt], str]: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è SAM3 –∏ reasoning –æ—Ç MLLM
        """
        image_b64 = self._image_to_base64(image)
        img_width, img_height = self._get_image_size(image)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è MLLM
        user_prompt = f"""Analyze this image and generate SAM3 prompts to segment: "{query}"

Please provide your response in JSON format with reasoning and prompts."""
        
        print(f"ü§ñ –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ —É Qwen VLM...")
        if self.debug:
            print(f"   Query: {query}")
        
        # –í—ã–∑—ã–≤–∞–µ–º Qwen image description —Å –Ω–∞—à–∏–º —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
        payload = {
            "image_base64": image_b64,
            "prompt": user_prompt,
            "detail_level": "comprehensive",
            "temperature": 0.7,
            "max_tokens": 2048
        }
        
        url = f"{self.qwen_url}/{self.qwen_api_version}/image/description"
        
        try:
            response = requests.post(url, json=payload, timeout=90)
            response.raise_for_status()
        except requests.exceptions.ConnectionError as e:
            raise ConnectionError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Qwen API –ø–æ –∞–¥—Ä–µ—Å—É {url}"
            ) from e
        
        result = response.json()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
        llm_response = result.get('result') or result.get('data') or {}
        if isinstance(llm_response, dict):
            llm_text = llm_response.get('description') or llm_response.get('text') or str(llm_response)
        else:
            llm_text = str(llm_response)
        
        print(f"üìù –û—Ç–≤–µ—Ç –æ—Ç Qwen:")
        print(f"{llm_text[:500]}...\n" if len(llm_text) > 500 else f"{llm_text}\n")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
        parsed_json = self._extract_json_from_text(llm_text)
        
        if parsed_json and 'prompts' in parsed_json:
            # MLLM –≤–µ—Ä–Ω—É–ª —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            reasoning = parsed_json.get('reasoning', 'No reasoning provided')
            prompts_data = parsed_json.get('prompts', [])
            
            prompts = []
            for p in prompts_data:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º bbox –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                box = p.get('box')
                if box:
                    try:
                        box = self._normalize_bbox_from_llm(box, img_width, img_height)
                    except ValueError as e:
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ bbox: {e}")
                        box = None
                
                prompt = SAM3Prompt(
                    type=p.get('type', 'text'),
                    text=p.get('text'),
                    box=box,
                    points=p.get('points'),
                    point_labels=p.get('point_labels'),
                    reasoning=reasoning
                )
                prompts.append(prompt)
            
            return prompts, reasoning
        else:
            # MLLM –Ω–µ –≤–µ—Ä–Ω—É–ª JSON, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            print("‚ö†Ô∏è  MLLM –Ω–µ –≤–µ—Ä–Ω—É–ª —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç")
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–≤–µ—Ç–∞
            prompt = SAM3Prompt(
                type="text",
                text=query,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                reasoning=f"Fallback: using original query as text prompt. LLM response: {llm_text[:200]}"
            )
            
            return [prompt], llm_text
    
    def segment_with_sam3(
        self,
        image: Union[str, Image.Image, np.ndarray],
        prompts: List[SAM3Prompt],
        confidence_threshold: float = 0.5
    ) -> List[SegmentationResult]:
        """
        –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å SAM3 –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–æ–º–ø—Ç—ã –æ—Ç MLLM
        
        Args:
            image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ –æ—Ç MLLM
            confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        """
        image_b64 = self._image_to_base64(image)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º SAM3Prompts –≤ —Ñ–æ—Ä–º–∞—Ç API
        sam3_prompts = []
        for p in prompts:
            if p.type == "text" and p.text:
                sam3_prompts.append({"type": "text", "text": p.text})
            elif p.type == "box" and p.box:
                sam3_prompts.append({"type": "box", "box": p.box, "label": True})
            elif p.type == "point" and p.points:
                sam3_prompts.append({
                    "type": "point",
                    "points": p.points,
                    "point_labels": p.point_labels or [1] * len(p.points)
                })
        
        if not sam3_prompts:
            print("‚ö†Ô∏è  –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è SAM3")
            return []
        
        payload = {
            "image": image_b64,
            "prompts": sam3_prompts,
            "confidence_threshold": confidence_threshold
        }
        
        url = f"{self.sam3_url}/api/{self.sam3_api_version}/image/segment"
        
        print(f"üéØ –û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ SAM3...")
        print(f"   –ü—Ä–æ–º–ø—Ç–æ–≤: {len(sam3_prompts)}")
        for i, p in enumerate(sam3_prompts):
            print(f"   [{i+1}] type={p['type']}, ", end="")
            if 'text' in p:
                print(f"text='{p['text']}'")
            elif 'box' in p:
                print(f"box={p['box']}")
            elif 'points' in p:
                print(f"points={len(p['points'])} points")
        
        try:
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
        except requests.exceptions.ConnectionError as e:
            raise ConnectionError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ SAM3 API –ø–æ –∞–¥—Ä–µ—Å—É {url}"
            ) from e
        
        result = response.json()
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        segmentations = []
        for i in range(result['num_masks']):
            bbox_list = result['boxes'][i]
            bbox = BoundingBox(
                cx=bbox_list[0],
                cy=bbox_list[1],
                w=bbox_list[2],
                h=bbox_list[3]
            )
            
            seg = SegmentationResult(
                mask=result['masks'][i],
                bbox=bbox,
                score=result['scores'][i],
                query="",  # –ó–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
                prompt_used=prompts[i] if i < len(prompts) else None
            )
            segmentations.append(seg)
        
        return segmentations
    
    def segment(
        self,
        image: Union[str, Image.Image, np.ndarray],
        query: str,
        confidence_threshold: float = 0.5
    ) -> List[SegmentationResult]:
        """
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω SAM3 Agent:
        1. MLLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –∑–∞–ø—Ä–æ—Å
        2. MLLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è SAM3
        3. SAM3 —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤
        
        Args:
            image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "the leftmost child wearing blue vest")
            confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è SAM3
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –æ—Ç MLLM
        """
        print("=" * 70)
        print(f"ü§ñ SAM3 Agent - –ê–Ω–∞–ª–∏–∑ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
        print(f"   Query: {query}")
        print("=" * 70)
        
        # –®–∞–≥ 1: MLLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã
        prompts, reasoning = self.generate_sam3_prompts(image, query)
        
        print(f"\nüí° Reasoning –æ—Ç MLLM:")
        print(f"{reasoning}\n")
        
        if not prompts:
            print("‚ö†Ô∏è  MLLM –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –ø—Ä–æ–º–ø—Ç–æ–≤")
            return []
        
        # –®–∞–≥ 2: SAM3 —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç
        segmentations = self.segment_with_sam3(
            image=image,
            prompts=prompts,
            confidence_threshold=confidence_threshold
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        for seg in segmentations:
            seg.query = query
            seg.llm_reasoning = reasoning
        
        print(f"\n‚úÖ –°–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(segmentations)}")
        for i, seg in enumerate(segmentations):
            print(f"   [{i+1}] score={seg.score:.2f}, bbox={seg.bbox.to_list()}")
            if seg.prompt_used:
                print(f"        prompt_type={seg.prompt_used.type}")
        
        print("=" * 70)
        
        return segmentations
    
    def visualize_results(
        self,
        image: Union[str, Image.Image],
        results: List[SegmentationResult],
        output_path: str = "output.jpg"
    ):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        
        Args:
            image: –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if isinstance(image, str):
            img = Image.open(image).convert('RGB')
        else:
            img = image.copy().convert('RGB')
        
        draw = ImageDraw.Draw(img)
        width, height = img.size
        
        # –†–∏—Å—É–µ–º bbox –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        colors = ['red', 'green', 'blue', 'yellow', 'magenta', 'cyan', 'orange', 'purple']
        
        for i, result in enumerate(results):
            color = colors[i % len(colors)]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π bbox –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            x1, y1, x2, y2 = result.bbox.to_xyxy(width, height)
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            x1 = max(0, min(x1, width))
            y1 = max(0, min(y1, height))
            x2 = max(0, min(x2, width))
            y2 = max(0, min(y2, height))
            
            if self.debug:
                print(f"   –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–∞ {i+1}:")
                print(f"     –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π bbox: {result.bbox.to_list()}")
                print(f"     –ê–±—Å–æ–ª—é—Ç–Ω—ã–π bbox: ({x1}, {y1}, {x2}, {y2})")
                print(f"     –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {width}x{height}")
            
            # –†–∏—Å—É–µ–º bbox
            draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å —Ñ–æ–Ω–æ–º –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
            text = f"#{i+1}: {result.score:.2f}"
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —à—Ä–∏—Ñ—Ç, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
            except:
                font = ImageFont.load_default()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            # –†–∏—Å—É–µ–º —Ñ–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            text_bg_y = max(0, y1 - text_height - 8)
            draw.rectangle(
                [x1, text_bg_y, x1 + text_width + 8, text_bg_y + text_height + 8],
                fill=color
            )
            
            # –†–∏—Å—É–µ–º —Ç–µ–∫—Å—Ç
            draw.text((x1 + 4, text_bg_y + 4), text, fill='white', font=font)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        img.save(output_path)
        print(f"üíæ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
        print(f"   –†–∞–∑–º–µ—Ä: {width}x{height}")
        print(f"   –û–±—ä–µ–∫—Ç–æ–≤: {len(results)}")
        
        return output_path


# ==================== –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ====================

def example_complex_query():
    """–ü—Ä–∏–º–µ—Ä —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º (–∫–∞–∫ –≤ sam3_agent.ipynb)"""
    print("=" * 60)
    print("–ü–†–ò–ú–ï–†: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ SAM3 Agent")
    print("=" * 60)
    
    agent = SAM3QwenAgent(
        sam3_url="http://localhost:8000",
        qwen_url="http://localhost:8001",
        debug=True
    )
    
    # –ü—Ä–∏–º–µ—Ä—ã –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    test_cases = [
        {
            "image": "/home/golovanks/projects/sgp_kras/MainHanlder/CT/anno_agent-main/tmp_cvat_download/images/00001.jpg",
            "query": "the largest container in the image"
        },
        {
            "image": "/home/golovanks/projects/sgp_kras/MainHanlder/CT/anno_agent-main/tmp_cvat_download/images/00001.jpg",
            "query": "all containers"
        },
        {
            "image": "/home/golovanks/projects/sgp_kras/MainHanlder/CT/anno_agent-main/tmp_cvat_download/images/00001.jpg",
            "query": "the container on the left side"
        }
    ]
    
    for i, test in enumerate(test_cases[:1]):  # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        print(f"\n{'='*60}")
        print(f"Test Case {i+1}")
        print(f"{'='*60}")
        
        try:
            results = agent.segment(
                image=test["image"],
                query=test["query"],
                confidence_threshold=0.5
            )
            
            if results:
                # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º
                output_path = f"output_test_{i+1}.jpg"
                agent.visualize_results(test["image"], results, output_path)
                
                print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è '{test['query']}':")
                for j, result in enumerate(results):
                    print(f"\n  –û–±—ä–µ–∫—Ç {j+1}:")
                    print(f"    Score: {result.score:.3f}")
                    print(f"    BBox: {result.bbox.to_list()}")
                    if result.prompt_used:
                        print(f"    Prompt type: {result.prompt_used.type}")
                        if result.prompt_used.text:
                            print(f"    Prompt text: {result.prompt_used.text}")
            else:
                print("  ‚ö†Ô∏è  –û–±—ä–µ–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()


def example_simple():
    """–ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä"""
    print("=" * 60)
    print("–ü–†–ò–ú–ï–†: –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å")
    print("=" * 60)
    
    agent = SAM3QwenAgent()
    
    try:
        results = agent.segment(
            image="/home/golovanks/projects/sgp_kras/MainHanlder/CT/anno_agent-main/tmp_cvat_download/images/00001.jpg",
            query="container",
            confidence_threshold=0.5
        )
        
        if results:
            print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(results)}")
            agent.visualize_results(
                "/home/golovanks/projects/sgp_kras/MainHanlder/CT/anno_agent-main/tmp_cvat_download/images/00001.jpg",
                results,
                "simple_output.jpg"
            )
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    """
    SAM3 Agent - MLLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç SAM3 –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
    
    –ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:
    1. SAM3 API –∑–∞–ø—É—â–µ–Ω –Ω–∞ http://localhost:8000
    2. Qwen VLM API –∑–∞–ø—É—â–µ–Ω –Ω–∞ http://localhost:8001
    """
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–º–µ—Ä —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
    example_complex_query()
    
    # –ò–ª–∏ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä
    # example_simple()
